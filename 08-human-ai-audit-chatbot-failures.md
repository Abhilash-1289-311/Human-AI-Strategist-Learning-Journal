# 08 - Human+AI Audit: Sneaky Chatbot Failures 🕵️‍♂️🤖

How even simple banking chatbots can make dangerous mistakes—and how a Human+AI Strategist catches them!

---

## Audit Scenario

**Company:** BigBank Ltd.  
**AI System:** 24/7 customer chatbot for banking queries

---

### Failure #1: Hallucination (Making Up Answers)

- **What Happened:**  
  Chatbot tells customers the wrong interest rate for personal loans because it pulls info from outdated FAQs.
- **Red Flag:**  
  Hallucination, No transparency, Legal risk
- **How To Fix:**  
  - Human expert regularly reviews AI answers for accuracy
  - Add a "human review" button for users
  - Log and escalate all finance-related answers for audit

---

### Failure #2: Bias (Unfair Treatment)

- **What Happened:**  
  AI recommends savings account upgrades more often to male customers than female customers.
- **Red Flag:**  
  Bias, Ethics violation, Affects vulnerable groups
- **How To Fix:**  
  - Regular human bias audits on AI decision data
  - Set up automated alerts for uneven patterns
  - Require human approval for sensitive recommendations

---

### Failure #3: Overconfidence

- **What Happened:**  
  Chatbot gives very confident but incorrect investment advice, leading to confusion and potential losses.
- **Red Flag:**  
  Overconfidence, High stakes/uncertainty
- **How To Fix:**  
  - Force chatbot to flag uncertain answers to humans
  - Add clear disclaimers for non-verified advice
  - Random human QA (quality assurance) checks

---

## Key Takeaway

> **Every major chatbot system must keep real banking experts "in-the-loop"—periodically checking answers, correcting unfairness, and making sure the AI doesn't harm users. Safety = trust.**

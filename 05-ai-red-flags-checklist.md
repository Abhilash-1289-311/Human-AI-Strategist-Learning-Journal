# 05 - AI Red Flags: When to Always Use Human Oversight ðŸš©ðŸ¤–

A simple checklist for catching risky AI moments before they cause trouble!

---

## When Should You Keep a Human in the Loop?

Use this checklist to spot danger zones in any AI workflow:

- â“ **Unclear or high-stakes decisions**
    - The outcome seriously affects money, health, freedom, or safety.
- ðŸ‘©â€âš–ï¸ **Legal or ethical consequences**
    - Breaking rules could get the company sued or fined.
- ðŸ™… **Bias or unfair treatment**
    - AI could make decisions that are unfair or discriminatory.
- ðŸ‘€ **No transparency**
    - Canâ€™t explain why the AI made a decision.
- ðŸ¥¸ **Impacts on privacy**
    - AI might share or misuse private/personal data.
- ðŸŽ° **High uncertainty**
    - AI guesses, shows low confidence, or â€œhallucinatesâ€ answers.
- ðŸŒŸ **Rare or edge cases**
    - New/unusual situations that werenâ€™t in the training data.
- ðŸ§‘â€ðŸ¦½ **Affects vulnerable groups**
    - People with disabilities, minorities, or at-risk communities are impacted.

---

## How This Checklist is Used

**Example:**  
A hospital is deploying an AI to diagnose patients.  
**Checklist use:**  
- High-stakes: YESâ€”add human doctors!
- Bias possible: YESâ€”monitor decisions!
- Unclear AI choices: YESâ€”require second review!

By running this checklist, the hospital adds humans at all the dangerous stepsâ€”protecting patients and the organization.

---

> **Remember:**  
> Any time you see a ðŸš© red flag, design the workflow so humans can check, double-check, or override the AI.

---

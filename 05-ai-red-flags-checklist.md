# 05 - AI Red Flags: When to Always Use Human Oversight 🚩🤖

A simple checklist for catching risky AI moments before they cause trouble!

---

## When Should You Keep a Human in the Loop?

Use this checklist to spot danger zones in any AI workflow:

- ❓ **Unclear or high-stakes decisions**
    - The outcome seriously affects money, health, freedom, or safety.
- 👩‍⚖️ **Legal or ethical consequences**
    - Breaking rules could get the company sued or fined.
- 🙅 **Bias or unfair treatment**
    - AI could make decisions that are unfair or discriminatory.
- 👀 **No transparency**
    - Can’t explain why the AI made a decision.
- 🥸 **Impacts on privacy**
    - AI might share or misuse private/personal data.
- 🎰 **High uncertainty**
    - AI guesses, shows low confidence, or “hallucinates” answers.
- 🌟 **Rare or edge cases**
    - New/unusual situations that weren’t in the training data.
- 🧑‍🦽 **Affects vulnerable groups**
    - People with disabilities, minorities, or at-risk communities are impacted.

---

## How This Checklist is Used

**Example:**  
A hospital is deploying an AI to diagnose patients.  
**Checklist use:**  
- High-stakes: YES—add human doctors!
- Bias possible: YES—monitor decisions!
- Unclear AI choices: YES—require second review!

By running this checklist, the hospital adds humans at all the dangerous steps—protecting patients and the organization.

---

> **Remember:**  
> Any time you see a 🚩 red flag, design the workflow so humans can check, double-check, or override the AI.

---
